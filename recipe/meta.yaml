{% set version = "2.1.0" %}
# Build 1 was only ever built for cuda 12.0
# Unskip other things when it passes
{% set build = 1 %}

{% if cuda_compiler_version != "None" %}
{% set build = build + 200 %}
{% endif %}

{% if blas_impl == "mkl" %}
{% set build = build + 100 %}
{% endif %}

{% if cuda_compiler_version in (None, "None", True, False) %}
{% set cuda_major = 0 %}
{% else %}
{% set cuda_major = environ.get("cuda_compiler_version", "11.8").split(".")[0] | int %}
{% endif %}

package:
  name: pytorch-recipe
  version: {{ version }}

source:
  # for local testing use a tarball including submodules
  git_url: https://github.com/pytorch/pytorch.git
  git_tag: v{{ version }}
  patches:
    - patches/nvml.patch  # [cuda_compiler_version == "11.2"]
    # The patch below is probably OK for other versions too, but it is untested with them
    # https://github.com/conda-forge/pytorch-cpu-feedstock/pull/203#issuecomment-1797352452
    # https://github.com/pytorch/pytorch/pull/82695/files#diff-8e5cb190cc46be808993381a31fe9c027705d356b6bc0460368c0310ae82b273R61-R66
    # https://github.com/pytorch/pytorch/pull/108932
    - patches/0001-Allow-splayed-layouts.patch  # [cuda_compiler_version == "12.0"]

build:
  number: {{ build }}
  skip: true  # [win]
  skip: true  # [cuda_compiler_version != "None" and blas_impl != "mkl"]
  skip: true  # [cuda_compiler_version == "10.2"]
  skip: true  # [cuda_compiler_version == "11.0"]
  skip: true  # [cuda_compiler_version == "11.1"]
  skip: true  # [cuda_compiler_version != "12.0"]

outputs:
  - name: pytorch
    build:
      string: cuda{{ cuda_compiler_version | replace('.', '') }}py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [cuda_compiler_version != "None"]
      string: cpu_{{ blas_impl }}_py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                                # [cuda_compiler_version == "None"]
      detect_binary_files_with_prefix: false
      run_exports:
        - {{ pin_subpackage('pytorch', max_pin='x.x') }}
    script: build_pytorch.sh  # [not win]
    script: bld_pytorch.bat   # [win]
    requirements:
      build:
        - python                                 # [build_platform != target_platform]
        - cross-python_{{ target_platform }}     # [build_platform != target_platform]
        - numpy                                  # [build_platform != target_platform]
        - sysroot_linux-64  2.17  # [linux64]
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}    # [cuda_compiler_version != "None"]
        {% if cuda_major >= 12 %}
        - cuda-driver-dev                        # [build_platform != target_platform]
        - cuda-cudart-dev                        # [build_platform != target_platform]
        - cuda-nvrtc-dev                         # [build_platform != target_platform]
        - cuda-nvtx-dev                          # [build_platform != target_platform]
        - cuda-nvml-dev                          # [build_platform != target_platform]
        - cuda-profiler-api                      # [build_platform != target_platform]
        - libcublas-dev                          # [build_platform != target_platform]
        - libcufft-dev                           # [build_platform != target_platform]
        - libcurand-dev                          # [build_platform != target_platform]
        - libcusolver-dev                        # [build_platform != target_platform]
        - libcusparse-dev                        # [build_platform != target_platform]
        {% endif %}
        # Dec 2020: it seems that git is broken on windows, so we use m2-git
        - patch     # [not win]
        - m2-patch  # [win]
        - git       # [not win]
        - m2-git    # [win]
        - libgomp   # [linux]
        - llvm-openmp    # [osx]
        - cmake
        - ninja
        # Keep libprotobuf here so that a compatibile version
        # of protobuf is installed between build and host
        - libprotobuf
        - protobuf
        - make      # [linux]
      host:
        # GPU requirements
        - cudnn                           # [cuda_compiler_version != "None"]
        - nccl                            # [cuda_compiler_version != "None"]
        - magma                           # [cuda_compiler_version != "None"]
        - cuda-version {{ cuda_compiler_version }}  # [cuda_compiler_version != "None"]
        {% if cuda_major >= 12 %}
        - cuda-driver-dev
        - cuda-cudart-dev
        - cuda-nvrtc-dev
        - cuda-nvtx-dev
        - cuda-nvml-dev
        - cuda-profiler-api
        - libcublas-dev
        - libcufft-dev
        - libcurand-dev
        - libcusolver-dev
        - libcusparse-dev
        {% endif %}
        # other requirements
        - python
        - numpy
        - pip
        - setuptools
        - pyyaml
        - requests
        - future
        - six
        - mkl-devel {{ mkl }}   # [blas_impl == "mkl"]
        - libcblas * *_mkl      # [blas_impl == "mkl"]
        - libcblas              # [blas_impl != "mkl"]
        - liblapack             # [blas_impl != "mkl"]
        - libgomp   # [linux]
        - llvm-openmp    # [osx]
        - libprotobuf
        - sleef
        - typing
        - libuv
        - pkg-config  # [unix]
        - typing_extensions
      run:
        - llvm-openmp    # [osx]
        # GPU requirements without run_exports
        - {{ pin_compatible('cudnn') }}                       # [cuda_compiler_version != "None"]
        - {{ pin_compatible('magma', max_pin='x.x.x') }}      # [cuda_compiler_version != "None"]
        # other requirements
        - python
        - typing_extensions
        - sympy
        - filelock
        - jinja2
        - networkx
        - nomkl                 # [blas_impl != "mkl"]
        - fsspec
        # avoid that people without GPUs needlessly download ~0.5-1GB
        - __cuda  # [cuda_compiler_version != "None"]
      run_constrained:
        # These constraints ensure conflict between pytorch and
        # pytorch-cpu 1.1 which we built before conda-forge had GPU infrastructure
        # built into place.
        # https://github.com/conda-forge/pytorch-cpu-feedstock/issues/65
        - pytorch-cpu =={{ version }}  # [cuda_compiler_version == "None"]
        - pytorch-gpu ==99999999       # [cuda_compiler_version == "None"]
        - pytorch-gpu =={{ version }}  # [cuda_compiler_version != "None"]
        - pytorch-cpu ==99999999       # [cuda_compiler_version != "None"]

    test:
      requires:
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - hypothesis
        - pytest
        - tabulate
        - pydot
        - mock  # [linux]
        - pip
        - expecttest
        - xmlrunner
      imports:
        - torch
      source_files:
        - test
        # tools/ is needed to optimise test run
        # as of pytorch=2.0.0, there is a bug when trying to run tests without the tools
        - tools
      commands:
        - OMP_NUM_THREADS=4 python ./test/run_test.py || true  # [not win]
        - python ./test/run_test.py  # [win]
        # Run pip check so as to ensure that all pytorch packages are installed
        # https://github.com/conda-forge/pytorch-cpu-feedstock/issues/24
        - pip check
        - python -c "import torch; print(torch.__version__)"
        - python -c "import torch; assert torch.backends.mkldnn.m.is_available()"  # [x86 and cuda_compiler_version == "None"]
        # At conda-forge, we target versions of OSX that are too old for MPS support
        # But if users install a newer version of OSX, they will have MPS support
        # https://github.com/conda-forge/pytorch-cpu-feedstock/pull/123#issuecomment-1186355073
        # - python -c "import torch; assert torch.backends.mps.is_available()" # [osx]

  # 2021/08/01, hmaarrfk
  # While this seems like a roundabout way of defining the package name
  # It helps the linter avoid errors on a package not having tests.
  {% set pytorch_cpu_gpu = "pytorch-cpu" %}   # [cuda_compiler_version == "None"]
  {% set pytorch_cpu_gpu = "pytorch-gpu" %}   # [cuda_compiler_version != "None"]
  - name: {{ pytorch_cpu_gpu }}
    build:
      string: cuda{{ cuda_compiler_version | replace('.', '') }}py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [cuda_compiler_version != "None"]
      string: cpu_{{ blas_impl }}_py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                      # [cuda_compiler_version == "None"]
      detect_binary_files_with_prefix: False
      # weigh down cpu implementation and give cuda preference
      track_features:
        - pytorch-cpu                                      # [cuda_compiler_version == "None"]
    requirements:
      run:
        - {{ pin_subpackage("pytorch", exact=True) }}
    test:
      imports:
        - torch

about:
  home: https://pytorch.org/
  license: BSD-3-Clause
  license_family: BSD
  license_file:
    - LICENSE
    - NOTICE
    - third_party/pybind11/LICENSE
  summary: PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.

extra:
  recipe-maintainers:
    - hmaarrfk
    - sodre
    - benjaminrwilson
    - Tobias-Fischer
    - beckermr
  feedstock-name: pytorch-cpu
