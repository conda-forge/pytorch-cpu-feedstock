From 188ed6285d97bad14ba4da2e33d3394efd2f8a77 Mon Sep 17 00:00:00 2001
From: Eddie Yan <eddiey@nvidia.com>
Date: Tue, 5 Aug 2025 00:49:50 +0000
Subject: [PATCH 14/17] Attempt to fix torch.backends.cudnn.rnn import

torch.backends.cudnn module in order to expose the .conv.fp32_precision
and .rnn.fp32_precision settings. However, it fails to account for the
existing torch.backends.cudnn.rnn module, which if imported after leaves
us in a limbo state where the additional .rnn.fp32_precision property is
no longer accessible.

This PR is WIP and attempts to remedy this by propagating the hack and
replaces the RNN module with a similar PropertyModule replacement. There
is more than one wart, e.g., a duplicate ContextProp definition in
rnn.py as the original one in backends seems to be too strict in its
frozen flags check.
---
 test/test_cuda.py                |  2 ++
 torch/backends/cudnn/__init__.py |  3 ++-
 torch/backends/cudnn/rnn.py      | 40 +++++++++++++++++++++++++++++++-
 3 files changed, 43 insertions(+), 2 deletions(-)

diff --git a/test/test_cuda.py b/test/test_cuda.py
index 0ebfe192f8d..2aafc98064b 100644
--- a/test/test_cuda.py
+++ b/test/test_cuda.py
@@ -853,6 +853,7 @@ print(t.is_pinned())
             self.assertEqual(torch.backends.cudnn.rnn.fp32_precision, "none")
 
     @recover_orig_fp32_precision
+    @serialTest()
     def test_fp32_precision_with_float32_matmul_precision(self):
         torch.set_float32_matmul_precision("highest")
         self.assertEqual(torch.backends.cuda.matmul.fp32_precision, "ieee")
@@ -862,6 +863,7 @@ print(t.is_pinned())
         self.assertEqual(torch.backends.cuda.matmul.fp32_precision, "tf32")
 
     @recover_orig_fp32_precision
+    @serialTest()
     def test_invalid_status_for_legacy_api(self):
         torch.backends.cudnn.conv.fp32_precision = "none"
         torch.backends.cudnn.rnn.fp32_precision = "tf32"
diff --git a/torch/backends/cudnn/__init__.py b/torch/backends/cudnn/__init__.py
index 5cd6ec297c7..d5bb6926840 100644
--- a/torch/backends/cudnn/__init__.py
+++ b/torch/backends/cudnn/__init__.py
@@ -15,6 +15,8 @@ from torch.backends import (
     PropModule,
 )
 
+from . import rnn
+
 
 try:
     from torch._C import _cudnn
@@ -229,7 +231,6 @@ class CudnnModule(PropModule):
         torch._C._get_cudnn_allow_tf32, torch._C._set_cudnn_allow_tf32
     )
     conv = _FP32Precision("cuda", "conv")
-    rnn = _FP32Precision("cuda", "rnn")
     fp32_precision = ContextProp(
         _get_fp32_precision_getter("cuda", "all"),
         _set_fp32_precision_setter("cuda", "all"),
diff --git a/torch/backends/cudnn/rnn.py b/torch/backends/cudnn/rnn.py
index 0dc9ca80aa6..9281234ae3e 100644
--- a/torch/backends/cudnn/rnn.py
+++ b/torch/backends/cudnn/rnn.py
@@ -1,5 +1,13 @@
 # mypy: allow-untyped-defs
+import sys
+
+import torch._C
 import torch.cuda
+from torch.backends import (
+    _get_fp32_precision_getter,
+    _set_fp32_precision_setter,
+    PropModule,
+)
 
 
 try:
@@ -24,7 +32,7 @@ def get_cudnn_mode(mode):
         # pyrefly: ignore [missing-attribute]
         return int(_cudnn.RNNMode.gru)
     else:
-        raise Exception(f"Unknown mode: {mode}")  # noqa: TRY002
+        raise ValueError(f"Unknown mode: {mode}")  # noqa: TRY002
 
 
 # NB: We don't actually need this class anymore (in fact, we could serialize the
@@ -46,6 +54,20 @@ class Unserializable:
         self.inner = None
 
 
+# we would like to use ContextProp from backends here but the
+# frozen flags appears to be overzealous
+class ContextProp:
+    def __init__(self, getter, setter):
+        self.getter = getter
+        self.setter = setter
+
+    def __get__(self, obj, objtype):
+        return self.getter()
+
+    def __set__(self, obj, val):
+        self.setter(val)
+
+
 def init_dropout_state(dropout, train, dropout_seed, dropout_state):
     dropout_desc_name = "desc_" + str(torch.cuda.current_device())
     dropout_p = dropout if train else 0
@@ -67,3 +89,19 @@ def init_dropout_state(dropout, train, dropout_seed, dropout_state):
             )
     dropout_ts = dropout_state[dropout_desc_name].get()
     return dropout_ts
+
+
+class CudnnRNNModule(PropModule):
+    def __init__(self, m, name):
+        super().__init__(m, name)
+        self.m.Unserializable = Unserializable
+        self.m.get_cudnn_mode = get_cudnn_mode
+        self.m.init_dropout_state = init_dropout_state
+
+    fp32_precision = ContextProp(
+        _get_fp32_precision_getter("cuda", "rnn"),
+        _set_fp32_precision_setter("cuda", "rnn"),
+    )
+
+
+sys.modules[__name__] = CudnnRNNModule(sys.modules[__name__], __name__)
