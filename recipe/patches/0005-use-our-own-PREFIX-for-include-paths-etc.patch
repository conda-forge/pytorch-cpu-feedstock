From 967c8b365a12753e12931087b9f135eca2852f1c Mon Sep 17 00:00:00 2001
From: "H. Vetinari" <h.vetinari@gmx.com>
Date: Thu, 23 Jan 2025 22:58:14 +1100
Subject: [PATCH 05/17] use our own PREFIX for include paths etc.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Updated to use `sysconfig.get_config_vars("prefix")` per
https://github.com/conda-forge/pytorch-cpu-feedstock/issues/424
and https://github.com/conda-forge/pytorch-cpu-feedstock/issues/447.

Co-Authored-By: Michał Górny <mgorny@quansight.com>
Co-Authored-By: Tobias Fischer <info@tobiasfischer.info>
---
 torch/utils/cpp_extension.py | 34 ++++++++++++++++++++--------------
 1 file changed, 20 insertions(+), 14 deletions(-)

diff --git a/torch/utils/cpp_extension.py b/torch/utils/cpp_extension.py
index f29c382f0e3..d865df1684b 100644
--- a/torch/utils/cpp_extension.py
+++ b/torch/utils/cpp_extension.py
@@ -1567,31 +1567,37 @@ def include_paths(device_type: str = "cpu", torch_include_dirs=True) -> list[str
     Returns:
         A list of include path strings.
     """
-    paths = []
     lib_include = os.path.join(_TORCH_PATH, 'include')
+    # Account for conda prefix.
+    conda_pieces = [sysconfig.get_config_var("prefix")] + IS_WINDOWS * ["Library"] + ["include"]
+    conda_include = os.path.join(*conda_pieces)
+    paths = [
+        conda_include,
+    ]
     if torch_include_dirs:
         paths.extend([
             lib_include,
             # Remove this once torch/torch.h is officially no longer supported for C++ extensions.
+            os.path.join(conda_include, 'torch', 'csrc', 'api', 'include'),
             os.path.join(lib_include, 'torch', 'csrc', 'api', 'include'),
         ])
     if device_type == "cuda" and IS_HIP_EXTENSION:
         paths.append(os.path.join(lib_include, 'THH'))
         paths.append(_join_rocm_home('include'))
     elif device_type == "cuda":
-        cuda_home_include = _join_cuda_home('include')
-        # if we have the Debian/Ubuntu packages for cuda, we get /usr as cuda home.
-        # but gcc doesn't like having /usr/include passed explicitly
-        if cuda_home_include != '/usr/include':
-            paths.append(cuda_home_include)
-
-        # Support CUDA_INC_PATH env variable supported by CMake files
-        if (cuda_inc_path := os.environ.get("CUDA_INC_PATH", None)) and \
-                cuda_inc_path != '/usr/include':
-
-            paths.append(cuda_inc_path)
-        if CUDNN_HOME is not None:
-            paths.append(os.path.join(CUDNN_HOME, 'include'))
+        # we need to avoid re-including the torch headers here, which breaks cross-compilation;
+        # see https://github.com/conda-forge/pytorch-cpu-feedstock/issues/447
+        # OTOH, we want to help downstream packages find the CUDA bits they need which are in
+        #   $CONDA_PREFIX/targets/<arch>-linux/include
+        # Here, CONDA_PREFIX is correct both when building a pytorch-dependent package (as it
+        # points to `build:`, where nvcc brings in the target-specific headers) as well as in
+        # end-user environments, where the run-dep on {{ compiler("cuda") }} brings the same.
+        conda_prefix = os.environ.get("CONDA_PREFIX", "")
+        conda_cuda_include = os.path.join(
+            conda_prefix, "targets", "@CUDA_TARGET@", "include"
+        )
+        if os.path.exists(os.path.join(conda_cuda_include, "cuda_runtime_api.h")):
+            paths.append(conda_cuda_include)
     elif device_type == "xpu":
         paths.append(_join_sycl_home('include'))
         paths.append(_join_sycl_home('include', 'sycl'))
