From 97ef248e4150e8cace5f21c8f8fa9eb87f768ff0 Mon Sep 17 00:00:00 2001
From: "H. Vetinari" <h.vetinari@gmx.com>
Date: Thu, 23 Jan 2025 22:58:14 +1100
Subject: [PATCH 05/13] use our own PREFIX for include paths etc.
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

- Obviously, we want to reuse the conda-native dependencies we've already brought
  along as dependencies; aside from avoiding duplication, this is also necessary
  to avoid failures if system versions are being used (e.g. sleef).

- The pytorch builds in conda-forge symlink the headers in the python package
  `pytorch` from the expected location in site-packages back to `$PREFIX/include`
  where the actual headers are present from `libtorch`. Note that only C/C++ headers
  independent of python are in `$PREFIX/include`, the rest stays in site-packages.

- However, symlinks are not available on windows, and CMake may get confused as well,
  so we need to fix the CMake metadata in `TorchConfig.cmake.in` to account for the
  fact that most of the headers are in $PREFIX/include, not site-packages (see #479).

- For downstream cross-compilation via `torch/utils/cpp_extension.py`, we need to
  avoid re-including the torch-specific headers from both PREFIX and BUILD_PREFIX;
  at the same time, we keep target-specific CUDA paths to avoid that downstream
  packages have to do this manually (this needs some templating at install-time,
  see build.sh). For more background see feedstock issues #424, #447, #487

- The same concern applies to `torch/_inductor/cpp_builder.py`. Note that this is
  for inductor's JIT mode, not its AOT mode, for which the end user provides a
  <filename>_compile_flags.json file.

Co-Authored-By: Daniel Petry <dpetry@anaconda.com>
Co-Authored-By: Michał Górny <mgorny@quansight.com>
Co-Authored-By: Tobias Fischer <info@tobiasfischer.info>
---
 cmake/TorchConfig.cmake.in     | 13 ++++++++++---
 torch/_inductor/cpp_builder.py |  4 +++-
 torch/utils/cpp_extension.py   | 34 ++++++++++++++++++++--------------
 3 files changed, 33 insertions(+), 18 deletions(-)

diff --git a/cmake/TorchConfig.cmake.in b/cmake/TorchConfig.cmake.in
index abf5c814911..475dc3b88ce 100644
--- a/cmake/TorchConfig.cmake.in
+++ b/cmake/TorchConfig.cmake.in
@@ -53,9 +53,16 @@ else()
 endif()
 
 # Include directories.
-set(TORCH_INCLUDE_DIRS
-  ${TORCH_INSTALL_PREFIX}/include
-  ${TORCH_INSTALL_PREFIX}/include/torch/csrc/api/include)
+if(EXISTS "${TORCH_INSTALL_PREFIX}/include/torch/csrc/api/include")
+  # top-level include directory
+  set(TORCH_INCLUDE_DIRS
+    ${TORCH_INSTALL_PREFIX}/include/torch/csrc/api/include)
+else()
+  # site-packages include directory
+  set(TORCH_INCLUDE_DIRS
+    ${TORCH_INSTALL_PREFIX}/include
+    ${TORCH_INSTALL_PREFIX}/../../../../include/torch/csrc/api/include)
+endif()
 
 # Library dependencies.
 if(@BUILD_SHARED_LIBS@)
diff --git a/torch/_inductor/cpp_builder.py b/torch/_inductor/cpp_builder.py
index 6dd6e0d2b5c..fbfa3175836 100644
--- a/torch/_inductor/cpp_builder.py
+++ b/torch/_inductor/cpp_builder.py
@@ -1522,10 +1522,12 @@ def get_cpp_torch_options(
         + python_include_dirs
         + torch_include_dirs
         + omp_include_dir_paths
+        + [sysconfig.get_config_var('prefix') + '/include']
+        + [sysconfig.get_config_var('prefix') + '/targets/@CUDA_TARGET@/include']
     )
     cflags = sys_libs_cflags + omp_cflags
     ldflags = omp_ldflags
-    libraries_dirs = python_libraries_dirs + torch_libraries_dirs + omp_lib_dir_paths
+    libraries_dirs = python_libraries_dirs + torch_libraries_dirs + omp_lib_dir_paths + [sysconfig.get_config_var('prefix') + '/targets/@CUDA_TARGET@/lib/stubs']
     libraries = torch_libraries + omp_lib
     passthrough_args = (
         sys_libs_passthrough_args + isa_ps_args_build_flags + omp_passthrough_args
diff --git a/torch/utils/cpp_extension.py b/torch/utils/cpp_extension.py
index a63bff50d5e..7da14c2429c 100644
--- a/torch/utils/cpp_extension.py
+++ b/torch/utils/cpp_extension.py
@@ -1604,31 +1604,37 @@ def include_paths(device_type: str = "cpu", torch_include_dirs=True) -> list[str
     Returns:
         A list of include path strings.
     """
-    paths = []
     lib_include = os.path.join(_TORCH_PATH, 'include')
+    # Account for conda prefix.
+    conda_pieces = [sysconfig.get_config_var("prefix")] + IS_WINDOWS * ["Library"] + ["include"]
+    conda_include = os.path.join(*conda_pieces)
+    paths = [
+        conda_include,
+    ]
     if torch_include_dirs:
         paths.extend([
             lib_include,
             # Remove this once torch/torch.h is officially no longer supported for C++ extensions.
+            os.path.join(conda_include, 'torch', 'csrc', 'api', 'include'),
             os.path.join(lib_include, 'torch', 'csrc', 'api', 'include'),
         ])
     if device_type == "cuda" and IS_HIP_EXTENSION:
         paths.append(os.path.join(lib_include, 'THH'))
         paths.append(_join_rocm_home('include'))
     elif device_type == "cuda":
-        cuda_home_include = _join_cuda_home('include')
-        # if we have the Debian/Ubuntu packages for cuda, we get /usr as cuda home.
-        # but gcc doesn't like having /usr/include passed explicitly
-        if cuda_home_include != '/usr/include':
-            paths.append(cuda_home_include)
-
-        # Support CUDA_INC_PATH env variable supported by CMake files
-        if (cuda_inc_path := os.environ.get("CUDA_INC_PATH", None)) and \
-                cuda_inc_path != '/usr/include':
-
-            paths.append(cuda_inc_path)
-        if CUDNN_HOME is not None:
-            paths.append(os.path.join(CUDNN_HOME, 'include'))
+        # we need to avoid re-including the torch headers here, which breaks cross-compilation;
+        # see https://github.com/conda-forge/pytorch-cpu-feedstock/issues/447
+        # OTOH, we want to help downstream packages find the CUDA bits they need which are in
+        #   $CONDA_PREFIX/targets/<arch>-linux/include
+        # Here, CONDA_PREFIX is correct both when building a pytorch-dependent package (as it
+        # points to `build:`, where nvcc brings in the target-specific headers) as well as in
+        # end-user environments, where the run-dep on {{ compiler("cuda") }} brings the same.
+        conda_prefix = os.environ.get("CONDA_PREFIX", "")
+        conda_cuda_include = os.path.join(
+            conda_prefix, "targets", "@CUDA_TARGET@", "include"
+        )
+        if os.path.exists(os.path.join(conda_cuda_include, "cuda_runtime_api.h")):
+            paths.append(conda_cuda_include)
     elif device_type == "xpu":
         paths.append(_join_sycl_home('include'))
         paths.append(_join_sycl_home('include', 'sycl'))
