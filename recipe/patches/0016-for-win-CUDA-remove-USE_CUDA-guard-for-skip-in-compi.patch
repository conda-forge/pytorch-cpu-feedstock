From 72422632191044ce3c60e94a6c9384398a3780b9 Mon Sep 17 00:00:00 2001
From: "H. Vetinari" <h.vetinari@gmx.com>
Date: Fri, 27 Feb 2026 09:33:18 +1100
Subject: [PATCH 16/16] for win+CUDA, remove USE_CUDA guard for skip in
 compiled_autograd.h

upstream does not consider torch.compile supported on windows, apparently,
though issues only seem to arise in combination with CUDA. The fact that
the USE_CUDA symbol may not be defined in downstream builds then leads to
strange compilation errors (apparent conflict between `std::string`
inclusions for different ABIs or even from different stdlibs).

Suggested-By: Tobias Fischer <info@tobiasfischer.info>
---
 torch/csrc/dynamo/compiled_autograd.h | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/torch/csrc/dynamo/compiled_autograd.h b/torch/csrc/dynamo/compiled_autograd.h
index ca9eb3e638f..9a3aecaf73e 100644
--- a/torch/csrc/dynamo/compiled_autograd.h
+++ b/torch/csrc/dynamo/compiled_autograd.h
@@ -1107,7 +1107,7 @@ struct IValuePacker {
   // parsing. See torch::jit::toIValue for more information.
   static at::TypePtr packed_type() {
     // On windows CPU is support compiled autograd.
-#if defined(_WIN32) && (defined(USE_CUDA) || defined(USE_ROCM))
+#if defined(_WIN32)
     // NB: the if-constexpr usage triggers compilation errors on Windows
     // with certain compiler settings
     // (see https://github.com/pytorch/pytorch/pull/144707 for examples).
