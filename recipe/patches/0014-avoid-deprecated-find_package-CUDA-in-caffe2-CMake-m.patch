From e644304ce9c67c3f4185141dc603f8196e32a7cd Mon Sep 17 00:00:00 2001
From: "H. Vetinari" <h.vetinari@gmx.com>
Date: Thu, 30 Jan 2025 08:33:44 +1100
Subject: [PATCH 14/15] avoid deprecated `find_package(CUDA)` in caffe2 CMake
 metadata

vendor the not-available-anymore function torch_cuda_get_nvcc_gencode_flag from CMake
---
 caffe2/CMakeLists.txt      |  14 ++--
 cmake/Summary.cmake        |  10 +--
 cmake/TorchConfig.cmake.in |   2 +-
 cmake/public/cuda.cmake    |  48 ++++----------
 cmake/public/utils.cmake   | 127 +++++++++++++++++++++++++++++++++++++
 setup.py                   |   2 +-
 6 files changed, 153 insertions(+), 50 deletions(-)

diff --git a/caffe2/CMakeLists.txt b/caffe2/CMakeLists.txt
index fc858f3e0de..6f9f29dfd54 100644
--- a/caffe2/CMakeLists.txt
+++ b/caffe2/CMakeLists.txt
@@ -942,25 +942,25 @@ if(USE_ROCM)
         "$<$<COMPILE_LANGUAGE:CXX>:ATen/core/ATen_pch.h>")
   endif()
 elseif(USE_CUDA)
-  set(CUDA_LINK_LIBRARIES_KEYWORD PRIVATE)
+  set(CUDAToolkit_LINK_LIBRARIES_KEYWORD PRIVATE)
   list(APPEND Caffe2_GPU_SRCS ${GENERATED_CXX_TORCH_CUDA})
-  if(CUDA_SEPARABLE_COMPILATION)
+  if(CUDAToolkit_SEPARABLE_COMPILATION)
     # Separate compilation fails when kernels using `thrust::sort_by_key`
     # are linked with the rest of CUDA code. Workaround by linking them separately.
     add_library(torch_cuda ${Caffe2_GPU_SRCS} ${Caffe2_GPU_CU_SRCS})
-    set_property(TARGET torch_cuda PROPERTY CUDA_SEPARABLE_COMPILATION ON)
+    set_property(TARGET torch_cuda PROPERTY CUDAToolkit_SEPARABLE_COMPILATION ON)
 
     add_library(torch_cuda_w_sort_by_key OBJECT
         ${Caffe2_GPU_SRCS_W_SORT_BY_KEY}
         ${Caffe2_GPU_CU_SRCS_W_SORT_BY_KEY})
-    set_property(TARGET torch_cuda_w_sort_by_key PROPERTY CUDA_SEPARABLE_COMPILATION OFF)
+    set_property(TARGET torch_cuda_w_sort_by_key PROPERTY CUDAToolkit_SEPARABLE_COMPILATION OFF)
     target_link_libraries(torch_cuda PRIVATE torch_cuda_w_sort_by_key)
   else()
     add_library(torch_cuda
         ${Caffe2_GPU_SRCS} ${Caffe2_GPU_SRCS_W_SORT_BY_KEY}
         ${Caffe2_GPU_CU_SRCS} ${Caffe2_GPU_CU_SRCS_W_SORT_BY_KEY})
   endif()
-  set(CUDA_LINK_LIBRARIES_KEYWORD)
+  set(CUDAToolkit_LINK_LIBRARIES_KEYWORD)
   torch_compile_options(torch_cuda)  # see cmake/public/utils.cmake
   target_compile_definitions(torch_cuda PRIVATE USE_CUDA)
 
@@ -1009,12 +1009,12 @@ elseif(USE_CUDA)
         torch_cuda
     )
     if($ENV{ATEN_STATIC_CUDA})
-      if(CUDA_VERSION_MAJOR LESS_EQUAL 11)
+      if(CUDAToolkit_VERSION_MAJOR LESS_EQUAL 11)
         target_link_libraries(torch_cuda_linalg PRIVATE
             CUDA::cusolver_static
             ${CUDAToolkit_LIBRARY_DIR}/liblapack_static.a     # needed for libcusolver_static
         )
-      elseif(CUDA_VERSION_MAJOR GREATER_EQUAL 12)
+      elseif(CUDAToolkit_VERSION_MAJOR GREATER_EQUAL 12)
         target_link_libraries(torch_cuda_linalg PRIVATE
             CUDA::cusolver_static
             ${CUDAToolkit_LIBRARY_DIR}/libcusolver_lapack_static.a     # needed for libcusolver_static
diff --git a/cmake/Summary.cmake b/cmake/Summary.cmake
index d8a4bcf2191..faf3efcc4e5 100644
--- a/cmake/Summary.cmake
+++ b/cmake/Summary.cmake
@@ -76,7 +76,7 @@ function(caffe2_print_configuration_summary)
     message(STATUS "    USE_CUSPARSELT      : ${USE_CUSPARSELT}")
     message(STATUS "    USE_CUDSS           : ${USE_CUDSS}")
     message(STATUS "    USE_CUFILE          : ${USE_CUFILE}")
-    message(STATUS "    CUDA version        : ${CUDA_VERSION}")
+    message(STATUS "    CUDA version        : ${CUDAToolkit_VERSION}")
     message(STATUS "    USE_FLASH_ATTENTION : ${USE_FLASH_ATTENTION}")
     message(STATUS "    USE_MEM_EFF_ATTENTION : ${USE_MEM_EFF_ATTENTION}")
     if(${USE_CUDNN})
@@ -88,7 +88,7 @@ function(caffe2_print_configuration_summary)
     if(${USE_CUFILE})
       message(STATUS "    cufile library    : ${CUDA_cuFile_LIBRARY}")
     endif()
-    message(STATUS "    CUDA root directory : ${CUDA_TOOLKIT_ROOT_DIR}")
+    message(STATUS "    CUDA root directory : ${CUDAToolkit_ROOT}")
     message(STATUS "    CUDA library        : ${CUDA_cuda_driver_LIBRARY}")
     message(STATUS "    cudart library      : ${CUDA_cudart_LIBRARY}")
     message(STATUS "    cublas library      : ${CUDA_cublas_LIBRARY}")
@@ -108,12 +108,12 @@ function(caffe2_print_configuration_summary)
       message(STATUS "    cuDSS library       : ${__tmp}")
     endif()
     message(STATUS "    nvrtc               : ${CUDA_nvrtc_LIBRARY}")
-    message(STATUS "    CUDA include path   : ${CUDA_INCLUDE_DIRS}")
-    message(STATUS "    NVCC executable     : ${CUDA_NVCC_EXECUTABLE}")
+    message(STATUS "    CUDA include path   : ${CUDATookit_INCLUDE_DIRS}")
+    message(STATUS "    NVCC executable     : ${CUDATookit_NVCC_EXECUTABLE}")
     message(STATUS "    CUDA compiler       : ${CMAKE_CUDA_COMPILER}")
     message(STATUS "    CUDA flags          : ${CMAKE_CUDA_FLAGS}")
     message(STATUS "    CUDA host compiler  : ${CMAKE_CUDA_HOST_COMPILER}")
-    message(STATUS "    CUDA --device-c     : ${CUDA_SEPARABLE_COMPILATION}")
+    message(STATUS "    CUDA --device-c     : ${CUDATookit_SEPARABLE_COMPILATION}")
     message(STATUS "    USE_TENSORRT        : ${USE_TENSORRT}")
     if(${USE_TENSORRT})
       message(STATUS "      TensorRT runtime library: ${TENSORRT_LIBRARY}")
diff --git a/cmake/TorchConfig.cmake.in b/cmake/TorchConfig.cmake.in
index 8f2b2c30aee..c2b205a5cd8 100644
--- a/cmake/TorchConfig.cmake.in
+++ b/cmake/TorchConfig.cmake.in
@@ -126,7 +126,7 @@ if(@USE_CUDA@)
     find_library(CAFFE2_NVRTC_LIBRARY caffe2_nvrtc PATHS "${TORCH_INSTALL_PREFIX}/lib")
     list(APPEND TORCH_CUDA_LIBRARIES ${CAFFE2_NVRTC_LIBRARY})
   else()
-    set(TORCH_CUDA_LIBRARIES ${CUDA_NVRTC_LIB})
+    set(TORCH_CUDA_LIBRARIES CUDA::nvrtc)
   endif()
   if(TARGET torch::nvtoolsext)
     list(APPEND TORCH_CUDA_LIBRARIES torch::nvtoolsext)
diff --git a/cmake/public/cuda.cmake b/cmake/public/cuda.cmake
index 72e6d9d71c8..792e2ac78d3 100644
--- a/cmake/public/cuda.cmake
+++ b/cmake/public/cuda.cmake
@@ -26,8 +26,8 @@ if(NOT MSVC)
 endif()
 
 # Find CUDA.
-find_package(CUDA)
-if(NOT CUDA_FOUND)
+find_package(CUDAToolkit)
+if(NOT CUDAToolkit_FOUND)
   message(WARNING
     "PyTorch: CUDA cannot be found. Depending on whether you are building "
     "PyTorch or a PyTorch dependent library, the next warning / error will "
@@ -36,8 +36,6 @@ if(NOT CUDA_FOUND)
   return()
 endif()
 
-# Enable CUDA language support
-set(CUDAToolkit_ROOT "${CUDA_TOOLKIT_ROOT_DIR}")
 # Pass clang as host compiler, which according to the docs
 # Must be done before CUDA language is enabled, see
 # https://cmake.org/cmake/help/v3.15/variable/CMAKE_CUDA_HOST_COMPILER.html
@@ -56,24 +54,18 @@ if(CMAKE_VERSION VERSION_GREATER_EQUAL 3.12.0)
   cmake_policy(SET CMP0074 NEW)
 endif()
 
-find_package(CUDAToolkit REQUIRED)
+find_package(CUDAToolkit REQUIRED COMPONENTS cudart nvrtc REQUIRED)
 
 cmake_policy(POP)
 
-if(NOT CMAKE_CUDA_COMPILER_VERSION VERSION_EQUAL CUDAToolkit_VERSION)
-  message(FATAL_ERROR "Found two conflicting CUDA versions:\n"
-                      "V${CMAKE_CUDA_COMPILER_VERSION} in '${CUDA_INCLUDE_DIRS}' and\n"
-                      "V${CUDAToolkit_VERSION} in '${CUDAToolkit_INCLUDE_DIRS}'")
-endif()
-
-message(STATUS "PyTorch: CUDA detected: " ${CUDA_VERSION})
-message(STATUS "PyTorch: CUDA nvcc is: " ${CUDA_NVCC_EXECUTABLE})
-message(STATUS "PyTorch: CUDA toolkit directory: " ${CUDA_TOOLKIT_ROOT_DIR})
-if(CUDA_VERSION VERSION_LESS 11.0)
+message(STATUS "PyTorch: CUDA detected: " ${CUDAToolkit_VERSION})
+message(STATUS "PyTorch: CUDA nvcc is: " ${CUDAToolkit_NVCC_EXECUTABLE})
+message(STATUS "PyTorch: CUDA toolkit directory: " ${CUDAToolkit_ROOT})
+if(CUDAToolkit_VERSION VERSION_LESS 11.0)
   message(FATAL_ERROR "PyTorch requires CUDA 11.0 or above.")
 endif()
 
-if(CUDA_FOUND)
+if(CUDAToolkit_FOUND)
   # Sometimes, we may mismatch nvcc with the CUDA headers we are
   # compiling with, e.g., if a ccache nvcc is fed to us by CUDA_NVCC_EXECUTABLE
   # but the PATH is not consistent with CUDA_HOME.  It's better safe
@@ -97,8 +89,8 @@ if(CUDA_FOUND)
     )
   if(NOT CMAKE_CROSSCOMPILING)
     try_run(run_result compile_result ${PROJECT_RANDOM_BINARY_DIR} ${file}
-      CMAKE_FLAGS "-DINCLUDE_DIRECTORIES=${CUDA_INCLUDE_DIRS}"
-      LINK_LIBRARIES ${CUDA_LIBRARIES}
+      CMAKE_FLAGS "-DINCLUDE_DIRECTORIES=${CUDAToolkit_INCLUDE_DIRS}"
+      LINK_LIBRARIES ${CUDAToolkit_LIBRARIES}
       RUN_OUTPUT_VARIABLE cuda_version_from_header
       COMPILE_OUTPUT_VARIABLE output_var
       )
@@ -106,30 +98,14 @@ if(CUDA_FOUND)
       message(FATAL_ERROR "PyTorch: Couldn't determine version from header: " ${output_var})
     endif()
     message(STATUS "PyTorch: Header version is: " ${cuda_version_from_header})
-    if(NOT cuda_version_from_header STREQUAL ${CUDA_VERSION_STRING})
-      # Force CUDA to be processed for again next time
-      # TODO: I'm not sure if this counts as an implementation detail of
-      # FindCUDA
-      set(${cuda_version_from_findcuda} ${CUDA_VERSION_STRING})
-      unset(CUDA_TOOLKIT_ROOT_DIR_INTERNAL CACHE)
-      # Not strictly necessary, but for good luck.
-      unset(CUDA_VERSION CACHE)
-      # Error out
-      message(FATAL_ERROR "FindCUDA says CUDA version is ${cuda_version_from_findcuda} (usually determined by nvcc), "
-        "but the CUDA headers say the version is ${cuda_version_from_header}.  This often occurs "
-        "when you set both CUDA_HOME and CUDA_NVCC_EXECUTABLE to "
-        "non-standard locations, without also setting PATH to point to the correct nvcc.  "
-        "Perhaps, try re-running this command again with PATH=${CUDA_TOOLKIT_ROOT_DIR}/bin:$PATH.  "
-        "See above log messages for more diagnostics, and see https://github.com/pytorch/pytorch/issues/8092 for more details.")
-    endif()
   endif()
 endif()
 
 # ---[ CUDA libraries wrapper
 
 # find lbnvrtc.so
-set(CUDA_NVRTC_LIB "${CUDA_nvrtc_LIBRARY}" CACHE FILEPATH "")
-if(CUDA_NVRTC_LIB AND NOT CUDA_NVRTC_SHORTHASH)
+get_target_property(CUDA_NVRTC_LIB CUDA::nvrtc INTERFACE_LINK_LIBRARIES)
+if(NOT CUDA_NVRTC_SHORTHASH)
   find_package(Python COMPONENTS Interpreter)
   execute_process(
     COMMAND Python::Interpreter -c
diff --git a/cmake/public/utils.cmake b/cmake/public/utils.cmake
index 0c8e91ab7cf..52d6857c918 100644
--- a/cmake/public/utils.cmake
+++ b/cmake/public/utils.cmake
@@ -306,6 +306,133 @@ macro(torch_hip_get_arch_list store_var)
   string(REPLACE " " ";" ${store_var} "${_TMP}")
 endmacro()
 
+# torch_cuda_get_nvcc_gencode_flag is part of find_package(CUDA), but not find_package(CUDAToolkit);
+# vendor it from https://github.com/Kitware/CMake/blob/master/Modules/FindCUDA/select_compute_arch.cmake
+# but disable CUDA_DETECT_INSTALLED_GPUS
+################################################################################################
+# Function for selecting GPU arch flags for nvcc based on CUDA architectures from parameter list
+# Usage:
+#   SELECT_NVCC_ARCH_FLAGS(out_variable [list of CUDA compute archs])
+function(CUDA_SELECT_NVCC_ARCH_FLAGS out_variable)
+  set(CUDA_ARCH_LIST "${ARGN}")
+
+  if("X${CUDA_ARCH_LIST}" STREQUAL "X" )
+    set(CUDA_ARCH_LIST "Auto")
+  endif()
+
+  set(cuda_arch_bin)
+  set(cuda_arch_ptx)
+
+  if("${CUDA_ARCH_LIST}" STREQUAL "All")
+    set(CUDA_ARCH_LIST ${CUDA_KNOWN_GPU_ARCHITECTURES})
+  elseif("${CUDA_ARCH_LIST}" STREQUAL "Common")
+    set(CUDA_ARCH_LIST ${CUDA_COMMON_GPU_ARCHITECTURES})
+  elseif("${CUDA_ARCH_LIST}" STREQUAL "Auto")
+    # disabled, replaced by common architectures
+    # CUDA_DETECT_INSTALLED_GPUS(CUDA_ARCH_LIST)
+    # message(STATUS "Autodetected CUDA architecture(s): ${CUDA_ARCH_LIST}")
+    set(CUDA_ARCH_LIST ${CUDA_COMMON_GPU_ARCHITECTURES})
+  endif()
+
+  # Now process the list and look for names
+  string(REGEX REPLACE "[ \t]+" ";" CUDA_ARCH_LIST "${CUDA_ARCH_LIST}")
+  list(REMOVE_DUPLICATES CUDA_ARCH_LIST)
+  foreach(arch_name ${CUDA_ARCH_LIST})
+    set(arch_bin)
+    set(arch_ptx)
+    set(add_ptx FALSE)
+    # Check to see if we are compiling PTX
+    if(arch_name MATCHES "(.*)\\+PTX$")
+      set(add_ptx TRUE)
+      set(arch_name ${CMAKE_MATCH_1})
+    endif()
+    if(arch_name MATCHES "^([0-9]\\.[0-9](\\([0-9]\\.[0-9]\\))?)$")
+      set(arch_bin ${CMAKE_MATCH_1})
+      set(arch_ptx ${arch_bin})
+    else()
+      # Look for it in our list of known architectures
+      if(${arch_name} STREQUAL "Fermi")
+        set(arch_bin 2.0 "2.1(2.0)")
+      elseif(${arch_name} STREQUAL "Kepler+Tegra")
+        set(arch_bin 3.2)
+      elseif(${arch_name} STREQUAL "Kepler+Tesla")
+        set(arch_bin 3.7)
+      elseif(${arch_name} STREQUAL "Kepler")
+        set(arch_bin 3.0 3.5)
+        set(arch_ptx 3.5)
+      elseif(${arch_name} STREQUAL "Maxwell+Tegra")
+        set(arch_bin 5.3)
+      elseif(${arch_name} STREQUAL "Maxwell")
+        set(arch_bin 5.0 5.2)
+        set(arch_ptx 5.2)
+      elseif(${arch_name} STREQUAL "Pascal")
+        set(arch_bin 6.0 6.1)
+        set(arch_ptx 6.1)
+      elseif(${arch_name} STREQUAL "Volta")
+        set(arch_bin 7.0 7.0)
+        set(arch_ptx 7.0)
+      elseif(${arch_name} STREQUAL "Turing")
+        set(arch_bin 7.5)
+        set(arch_ptx 7.5)
+      elseif(${arch_name} STREQUAL "Ampere")
+        set(arch_bin 8.0)
+        set(arch_ptx 8.0)
+      else()
+        message(SEND_ERROR "Unknown CUDA Architecture Name ${arch_name} in CUDA_SELECT_NVCC_ARCH_FLAGS")
+      endif()
+    endif()
+    if(NOT arch_bin)
+      message(SEND_ERROR "arch_bin wasn't set for some reason")
+    endif()
+    list(APPEND cuda_arch_bin ${arch_bin})
+    if(add_ptx)
+      if (NOT arch_ptx)
+        set(arch_ptx ${arch_bin})
+      endif()
+      list(APPEND cuda_arch_ptx ${arch_ptx})
+    endif()
+  endforeach()
+
+  # remove dots and convert to lists
+  string(REGEX REPLACE "\\." "" cuda_arch_bin "${cuda_arch_bin}")
+  string(REGEX REPLACE "\\." "" cuda_arch_ptx "${cuda_arch_ptx}")
+  string(REGEX MATCHALL "[0-9()]+" cuda_arch_bin "${cuda_arch_bin}")
+  string(REGEX MATCHALL "[0-9]+"   cuda_arch_ptx "${cuda_arch_ptx}")
+
+  if(cuda_arch_bin)
+    list(REMOVE_DUPLICATES cuda_arch_bin)
+  endif()
+  if(cuda_arch_ptx)
+    list(REMOVE_DUPLICATES cuda_arch_ptx)
+  endif()
+
+  set(nvcc_flags "")
+  set(nvcc_archs_readable "")
+
+  # Tell NVCC to add binaries for the specified GPUs
+  foreach(arch ${cuda_arch_bin})
+    if(arch MATCHES "([0-9]+)\\(([0-9]+)\\)")
+      # User explicitly specified ARCH for the concrete CODE
+      list(APPEND nvcc_flags -gencode arch=compute_${CMAKE_MATCH_2},code=sm_${CMAKE_MATCH_1})
+      list(APPEND nvcc_archs_readable sm_${CMAKE_MATCH_1})
+    else()
+      # User didn't explicitly specify ARCH for the concrete CODE, we assume ARCH=CODE
+      list(APPEND nvcc_flags -gencode arch=compute_${arch},code=sm_${arch})
+      list(APPEND nvcc_archs_readable sm_${arch})
+    endif()
+  endforeach()
+
+  # Tell NVCC to add PTX intermediate code for the specified architectures
+  foreach(arch ${cuda_arch_ptx})
+    list(APPEND nvcc_flags -gencode arch=compute_${arch},code=compute_${arch})
+    list(APPEND nvcc_archs_readable compute_${arch})
+  endforeach()
+
+  string(REPLACE ";" " " nvcc_archs_readable "${nvcc_archs_readable}")
+  set(${out_variable}          ${nvcc_flags}          PARENT_SCOPE)
+  set(${out_variable}_readable ${nvcc_archs_readable} PARENT_SCOPE)
+endfunction()
+
 ##############################################################################
 # Get the XPU arch flags specified by TORCH_XPU_ARCH_LIST.
 # Usage:
diff --git a/setup.py b/setup.py
index 75fdfce7e35..93659023593 100644
--- a/setup.py
+++ b/setup.py
@@ -626,7 +626,7 @@ class build_ext(setuptools.command.build_ext.build_ext):
         else:
             report("-- Not using cuDNN")
         if cmake_cache_vars["USE_CUDA"]:
-            report("-- Detected CUDA at " + cmake_cache_vars["CUDA_TOOLKIT_ROOT_DIR"])
+            report(f"-- Detected CUDA at {cmake_cache_vars['CMAKE_CUDA_COMPILER_TOOLKIT_ROOT']}")
         else:
             report("-- Not using CUDA")
         if cmake_cache_vars["USE_XPU"]:
